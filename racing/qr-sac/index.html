<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Deep dive analysis of 'Motion Planning for Autonomous Vehicles in the Presence of Uncertainty Using Reinforcement Learning' - Research paper review">
    <title>Paper Review - Motion Planning for Autonomous Vehicles in the Presence of Uncertainty Using Reinforcement Learning</title>
    <link rel="stylesheet" href="./css/styles.css">
    <link rel="stylesheet" href="./css/paper-review.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <!-- Header with navigation -->
    <div class="header-sensor"></div>
    <header class="header">
        <div class="container">
            <div class="header-content">
                <div class="logo">
                    <svg class="logo-icon" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <rect width="40" height="40" rx="8" fill="#F4F0FF"/>
                        <path d="M20 8L32 14.5V27.5L20 34L8 27.5V14.5L20 8Z" stroke="#6E56CF" stroke-width="2"/>
                        <path d="M20 21V34M20 8V21M8 14.5L20 21L32 14.5" stroke="#6E56CF" stroke-width="2"/>
                        <circle cx="20" cy="21" r="3" fill="#6E56CF"/>
                    </svg>
                    <h1 class="logo-text">Paper Reviews</h1>
                </div>
                
                <button class="mobile-menu-toggle" aria-label="Toggle menu">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
                
                <nav class="nav">
                    <ul class="nav-links">
                        <li><a href="https://paper-reviews97.github.io" class="nav-link">Home</a></li>
                        <li><a href="../../../index.html#categories" class="nav-link">Categories</a></li>
                        <li><a href="https://paper-reviews97.github.io#featured" class="nav-link">Featured</a></li>
                        <li><a href="index.html#about" class="nav-link">About</a></li>
                    </ul>
                    <div class="nav-actions">
                        <a href="#" class="btn btn-primary">Subscribe</a>
                    </div>
                </nav>
            </div>
        </div>
    </header>

    <!-- Paper header -->
    <section class="paper-header">
        <div class="container">
            <div class="paper-header-content">
                <div class="breadcrumb">
                    <a href="./../../">Home</a>
                    <span class="breadcrumb-separator">/</span>
                    <a href="./../../index.html#categories">Categories</a>
                    <span class="breadcrumb-separator">/</span>
                    <a href="/">Racing</a>
                    <span class="breadcrumb-separator">/</span>
                    <span class="breadcrumb-current">Motion Planning for Autonomous Vehicles in the Presence of Uncertainty Using Reinforcement Learning</span>
                </div>
                
                <div class="paper-metadata">
                    <div class="paper-category-badge">
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path d="M12 2L2 7L12 12L22 7L12 2Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            <path d="M2 17L12 22L22 17" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            <path d="M2 12L12 17L22 12" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                        </svg>
                        <span>Racing</span>
                    </div>
                    <span class="paper-year">2021</span>
                    <span class="paper-journal">IROS</span>
                    <span class="paper-citation-count">27+ citations</span>
                </div>
                
                <h1 class="paper-title">Motion Planning for Autonomous Vehicles in the Presence of Uncertainty Using Reinforcement Learning</h1>
                <p class="paper-authors">Kasra Rezaee, Peyman Yadmellat, Simon Chamorro</p>
                
                <div class="paper-link-buttons">
                    <a href="https://ieeexplore.ieee.org/document/9636480?denied=" class="btn btn-secondary" target="_blank">
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            <path d="M15 3h6v6" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            <path d="M10 14L21 3" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                        </svg>
                        Original Paper
                    </a>
                    <button class="btn btn-outline share-btn" onclick="copyCurrentURL()">
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <circle cx="18" cy="5" r="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            <circle cx="6" cy="12" r="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            <circle cx="18" cy="19" r="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            <path d="M8.59 13.51L15.42 17.49" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            <path d="M15.41 6.51L8.59 10.49" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                        </svg>
                        Share Review
                    </button>
                </div>
                
                <div class="paper-highlights">
                    <div class="highlight">
                        <div class="highlight-icon">
                            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                                <path d="M12 15a3 3 0 100-6 3 3 0 000 6z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                                <path d="M19.4 15a1.65 1.65 0 00.33 1.82l.06.06a2 2 0 010 2.83 2 2 0 01-2.83 0l-.06-.06a1.65 1.65 0 00-1.82-.33 1.65 1.65 0 00-1 1.51V21a2 2 0 01-2 2 2 2 0 01-2-2v-.09A1.65 1.65 0 009 19.4a1.65 1.65 0 00-1.82.33l-.06.06a2 2 0 01-2.83 0 2 2 0 010-2.83l.06-.06a1.65 1.65 0 00.33-1.82 1.65 1.65 0 00-1.51-1H3a2 2 0 01-2-2 2 2 0 012-2h.09A1.65 1.65 0 004.6 9a1.65 1.65 0 00-.33-1.82l-.06-.06a2 2 0 010-2.83 2 2 0 012.83 0l.06.06a1.65 1.65 0 001.82.33H9a1.65 1.65 0 001-1.51V3a2 2 0 012-2 2 2 0 012 2v.09a1.65 1.65 0 001 1.51 1.65 1.65 0 001.82-.33l.06-.06a2 2 0 012.83 0 2 2 0 010 2.83l-.06.06a1.65 1.65 0 00-.33 1.82V9a1.65 1.65 0 001.51 1H21a2 2 0 012 2 2 2 0 01-2 2h-.09a1.65 1.65 0 00-1.51 1z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            </svg>
                        </div>
                        <div class="highlight-content">
                            <span class="highlight-label">Innovation Level</span>
                            <span class="highlight-value">Breakthrough</span>
                        </div>
                    </div>
                    <div class="highlight">
                        <div class="highlight-icon">
                            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                                <path d="M20.84 4.61a5.5 5.5 0 00-7.78 0L12 5.67l-1.06-1.06a5.5 5.5 0 00-7.78 7.78l1.06 1.06L12 21.23l7.78-7.78 1.06-1.06a5.5 5.5 0 000-7.78z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            </svg>
                        </div>
                        <div class="highlight-content">
                            <span class="highlight-label">Reviewer Rating</span>
                            <span class="highlight-value">98% Approval</span>
                        </div>
                    </div>
                    <div class="highlight">
                        <div class="highlight-icon">
                            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                                <path d="M22 12h-4l-3 9L9 3l-3 9H2" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            </svg>
                        </div>
                        <div class="highlight-content">
                            <span class="highlight-label">Impact Factor</span>
                            <span class="highlight-value">High</span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Review content -->
    <section class="paper-review-content">
        <div class="container">
            <div class="review-layout">
                <!-- Main content -->
                <div class="review-main">
                    <!-- Abstract card -->
                    <div class="review-card abstract-card">
                        <h2 class="review-section-title">Abstract</h2>
                        <div class="review-section-content">
                            <p>This paper addresses the challenge of motion planning under uncertainty—a critical issue for autonomous driving where sensor limitations, occlusions, and restricted fields of view can compromise safety. Rather than following traditional reinforcement learning (RL) approaches that optimize the average expected reward, the authors propose a method that leverages distributional RL to optimize for the worst-case outcome. By integrating quantile regression with established RL algorithms (namely Soft Actor-Critic and DQN), the approach seeks to produce a more conservative and safe policy. Evaluated in two simulation scenarios using the SUMO traffic simulator, the proposed method demonstrates improved collision avoidance and a behavior that approximates human driving under challenging conditions.</p>
                        </div>
                    </div>
                    
                    <!-- Key Takeaways -->
                    <div class="review-card">
                        <h2 class="review-section-title">Key Takeaways</h2>
                        <div class="review-section-content">
                            <div class="takeaways-grid">
                                <div class="takeaway-item">
                                    <div class="takeaway-icon">1</div>
                                    <div class="takeaway-content">
                                        <h3 class="takeaway-title">Worst-Case Reward Optimization</h3>
                                        <p class="takeaway-description">By reformulating the RL objective to maximize the lower bound of the reward distribution rather than the average, the paper presents a safety-centric approach that minimizes risky behaviors in uncertain environments.</p>
                                    </div>
                                </div>
                                
                                <div class="takeaway-item">
                                    <div class="takeaway-icon">2</div>
                                    <div class="takeaway-content">
                                        <h3 class="takeaway-title">Handling Occlusion and Sensing Uncertainty</h3>
                                        <p class="takeaway-description">The proposed method directly addresses uncertainties stemming from limited sensor ranges and occlusions—common issues in real-world autonomous driving—by incorporating them into the motion planning framework.</p>
                                    </div>
                                </div>
                                
                                <div class="takeaway-item">
                                    <div class="takeaway-icon">3</div>
                                    <div class="takeaway-content">
                                        <h3 class="takeaway-title">Integration of Quantile Regression with RL Algorithms</h3>
                                        <p class="takeaway-description">The study adapts standard RL algorithms (Soft Actor-Critic and DQN) using quantile regression. This modification enables the estimation of a reward distribution and supports the selection of actions based on a conservative evaluation of outcomes.</p>
                                    </div>
                                </div>

                                <div class="takeaway-item">
                                    <div class="takeaway-icon">4</div>
                                    <div class="takeaway-content">
                                        <h3 class="takeaway-title">Improved Safety and Performance in Simulation</h3>
                                        <p class="takeaway-description">Experimental results on both pedestrian crossing and curved road scenarios demonstrate that the conservative RL policies can reduce collision rates and achieve driving behaviors that are comparable to cautious human drivers, outperforming traditional RL and rule-based planners in critical metrics.</p>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                    
                    <!-- Review sections - Introduction -->
                    <div class="review-card" id="introduction">
                        <h2 class="review-section-title">Introduction</h2>
                        <div class="review-section-content">
                            <figure class="review-figure">
                                <img src="https://i.imgur.com/PcabN50.png" alt="intro" style="display: block; margin: 0 auto; width: 100%; max-width: 800px;" class="image-medium">
                                <figcaption>Figure 1. The figure illustrates two driving scenarios where occlusion affects autonomous vehicle visibility: in the pedestrian crossing scenario, a parked yellow vehicle at the corner occludes potential pedestrians approaching the crossing, while in the curved road scenario, the bend creates an occluded area (highlighted in red) that limits sensor coverage and may conceal an oncoming vehicle.</figcaption>
                            </figure>

                            <li><strong>Motion planning</strong> is defined as the task of determining a path for an autonomous vehicle to achieve its objectives.</li>
                            <li>The primary goal of <strong>motion planning</strong> is to ensure a safe trajectory.</li>
                            <li>The work concentrates on <strong>uncertainty</strong> arising from limitations in sensing and perception, specifically due to limited field of view, <strong>occlusion</strong>, and sensing range.</li>
                            <li>These <strong>uncertainties</strong> make achieving a safe trajectory a challenging task in the context of self-driving.</li>
                            <p>These points underscore the importance of robust motion planning algorithms that can handle real-world uncertainties to ensure the safety and reliability of autonomous vehicles. The paper will further delve into how RL can be leveraged to address these challenges.</p>
                        </div>
                    </div>
                    
                    <!-- Methodology -->
                    <div class="review-card" id="methodology">
                        <h2 class="review-section-title">Methodology</h2>
                        <div class="review-section-content">
                            <ol>
                                <!-- Distributional RL with Quantile Regression -->
                                <li><strong>Distributional RL with Quantile Regression</strong>: Instead of predicting a single Q-value for a state-action pair, the algorithm predicts a set of quantiles representing the distribution of possible returns. This is done using Quantile Regression.</li>
                                <ul>
                                    <li>Quantile Regression: For a given probability level τ (e.g., 0.1 for the 10th percentile), quantile regression aims to estimate the value qτ below which τ fraction of the data falls.  In this context, the algorithm learns to predict the quantiles of the return distribution. This is achieved by minimizing a loss function that penalizes underestimation and overestimation differently based on the chosen quantile level \(\tau\). The loss function used in quantile regression for quantile \(\tau\) is:</li>
                                    <p>\[L(u) = τ * u \; \text{if} \; u &gt; 0 \; L(u) = (τ - 1) * u \; \text{if} \; u &lt; 0\]</p>
                                    <p>where \(u\) is the residual between the predicted quantile and the target return.</p>
                                </ul>

                                <!-- Conservative Policy Optimization -->
                                <li><strong>Conservative Policy Optimization</strong>: The key modification is in how the Q-value is used for policy optimization. Instead of using the mean of the predicted quantiles (as in standard QR-DQN or a similar distributional SAC implementation), the algorithm uses the lowest quantile as the Q-value: </li>
                                <p>\[Q(s, a) = q_1(s, a)\]</p>
                                <p>This effectively forces the agent to optimize for the worst-case scenario, as it only considers the lowest possible return when evaluating a state-action pair.</p>

                                <!-- CQR-DQN -->
                                <li><strong>CQR-DQN</strong>:</li>
                                <ul>
                                    <li>The DQN algorithm is extended to predict \(N\) quantiles.</li>
                                    <li>The target Q-value for the DQN update is calculated using the lowest quantile, i.e., the target is \(q_1(s, a)\).</li>
                                    <li>The action selection (epsilon-greedy or similar) is also based on the Q-values derived from the lowest quantile.</li>
                                </ul>

                                <!-- CQR-SAC -->
                                <li><strong>CQR-SAC</strong>:</li>
                                <ul>
                                    <li>The Q-networks in SAC are modified to predict N quantiles.</li>
                                    <li>The distributional Bellman equation is used to update the Critic. The Bellman update is: \(Z(s, a) := r(s, a) + \gamma(Z(s^{\prime}, a^{\prime}) - log \pi(a^{\prime}|s^{\prime}))\), where \(Z\) represents the distribution of returns.</li>
                                    <li>The key difference is how \(Z(s, a)\) is estimated. Instead of a Gaussian distribution, the paper uses quantile regression to approximate the distribution.</li>
                                    <li>The Q-value used for the Actor update (to improve the policy) is also based on the lowest quantile. The Actor maximizes the Q-value derived from the lowest quantile. The actor update rule is kept the same as the original SAC.</li>
                                </ul>

                                <!-- Trajectory Evaluation vs. Policy Evaluation  -->
                                <li><strong>Trajectory Evaluation vs. Policy Evaluation</strong>: The paper distinguishes between two ways of evaluating the RL agent's performance,</li>
                                <ul>
                                    <li>rajectory evaluation assumes that the vehicle will follow the same trajectory into the future and does not adapt the trajectory as more information becomes available.</li>
                                    <li>Policy evaluation assumes that the agent chooses the next trajectory from its trained policy, thus allowing the vehicle to adapt the trajectory as more information becomes available.</li>
                                </ul>
                            </ol>

                            <figure class="review-figure">
                                <img src="https://i.imgur.com/lcEAMD9.png" alt="method2" style="display: block; margin: 0 auto; width: 100%; max-width: 800px;" class="image-medium">
                                <figcaption>Figure 2. The figure presents a variation of the classic Cliff Walk example from reinforcement learning, where an agent begins at start (S) and must reach goal (G) while avoiding a hazardous cliff—represented by a gray region that inflicts a -20 penalty and terminates the episode if entered—with three possible paths: Path 1 is the shortest yet most perilous due to its proximity to the cliff, Path 2 offers a safer but longer route, and Path 3 provides an alternative option; additionally, with probability p, any chosen action may be randomly replaced by a downward movement, and the agent incurs a -1 reward for each step, thereby illustrating the trade-off between maximizing expected reward and mitigating risk, and underscoring why standard reinforcement learning approaches might be inadequate for real-world, safety-critical problems, thus motivating the use of Distributional RL to optimize for worst-case outcomes.</figcaption>
                            </figure>
                        </div>
                    </div>
                    
                    <!-- Results -->
                    <div class="review-card" id="results">
                        <h2 class="review-section-title">Results</h2>
                        <div class="review-section-content">

                            <p>Evaluated in two simulated scenarios using SUMO:</p>
                            <ul>
                                <li>Pedestrian crossing with occlusion.</li>
                                <li>Curved road with occlusion.</li>
                            </ul>
                            <p>Compared CQR-SAC and CQR-DQN against standard SAC, DQN, QR-SAC, and QR-DQN, and rule-based planners (fixed speed, naive, aware).</p>
                            <p>Results showed that CQR-SAC and CQR-DQN achieved lower collision rates compared to the standard RL algorithms, demonstrating the effectiveness of optimizing for the worst-case scenario.  However, the performance of the algorithms depended on how the trajectory was evaluated.</p>
                            <p>The conservative algorithms learned to drive more cautiously in the presence of occlusions, resulting in safer behavior.</p>
                            <p>In the pedestrian crossing scenario, maximizing the lower bound of the reward resulted in overall better reward due to the lower speed and less penalizing brake.</p>

                            <!-- Figure 3 -->
                            <figure class="review-figure">
                                <img src="https://i.imgur.com/amsW5mk.png" alt="results1" class="image-medium">
                                <figcaption>Figure 3. The figure depicts the training progress of various reinforcement learning algorithms—including SAC, QR-SAC, CQR-SACπ, CQR-SAC\(tau\), DQN, QR-DQN, CQR-DQN\(\pi\), and CQR-DQN\(tau\) in two scenarios: a pedestrian crossing and a curved road. It shows the collision rate, average episode speed, and episode reward as functions of training steps. The collision rate decreases as training progresses, with the CQR-SAC\(\pi\) and CQR-DQN\(\pi\) algorithms generally achieving lower collision rates in line with the objective of optimizing for the worst-case scenario to enhance safety. Meanwhile, the average episode speed increases as the agents become more proficient at navigating the environment, and the episode reward rises, indicating that the agents are learning policies that yield higher cumulative rewards. The shaded regions around each performance line represent the variability across multiple training runs, providing insight into each algorithm’s robustness.</figcaption>
                            </figure>
                            
                            <!-- Figure 4 -->
                            <figure class="review-figure">
                                <img src="https://i.imgur.com/bTD4rks.png" alt="results2 class="image-medium">
                                <figcaption>Figure 4. This table summarizes the test performance of various algorithms in two scenarios—a curved road, which evaluates lane maintenance, speed, and collision avoidance, and a pedestrian crossing, which emphasizes safety and responsiveness. The metrics include average reward (r̄), collision rate (%), average speed (v̄ in m/s), and the 5th percentile acceleration (a in m/s²) that indicates deceleration intensity during critical events. The compared algorithms encompass traditional reinforcement learning approaches (SAC, DQN), their distributional RL extensions using quantile regression (QR-SAC, QR-DQN), and conservative variants (CQR-SACπ, CQR-DQNπ, CQR-SACτ, CQR-DQNτ), alongside rule-based planners (Fixed, Naive, Aware). Notably, the Aware planner achieves a 0% collision rate on the curved road, serving as a safety baseline, while the CQR-SACπ and CQR-DQNπ variants generally exhibit lower collision rates compared to SAC and DQN. Variations in average reward reflect different trade-offs among safety, comfort, and mobility, and the 5th percentile acceleration values reveal how aggressively the vehicle brakes during emergencies. The π variants assess the agent’s policy for greater flexibility, whereas the τ variants evaluate a specific trajectory for a more conservative approach. Overall, these results suggest that accounting for uncertainty and optimizing for the worst-case scenario—particularly with conservative QR-RL algorithms—can lead to safer motion planning in occluded scenarios, with rule-based planners providing a useful performance baseline.</figcaption>
                            </figure>

                            <!-- Figure 5 -->
                            <figure class="review-figure">
                                <img src="https://i.imgur.com/DC7FXza.png" alt="results3 class="image-medium">
                                <figcaption>Figure 5. The figure compares the behaviors of two motion planners—Soft Actor-Critic (SAC) and Conservative QR-SAC (CQR-SACπ)—in a simulated driving scenario where vehicle positions across multiple episodes are shown as colored dots indicating speed. In the top panel, the SAC planner displays a more dispersed distribution of dots that extend further into the intersection, with a color gradient revealing that the vehicle maintains higher speeds near the pedestrian crossing. In contrast, the bottom panel shows the CQR-SACπ planner forming a denser cluster of dots positioned further from the intersection, with colors indicating a significant reduction in speed as the vehicle approaches the crosswalk. Notably, the SAC planner tends to approach the crossing at higher speeds, resulting in less consistency in vehicle positions, while the CQR-SACπ planner exhibits a more cautious approach by slowing down considerably, thereby forming a tighter cluster of positions. Both planners also demonstrate a tendency to shift slightly to the left near the crosswalk—a behavior learned to improve visibility behind occlusions. Overall, the CQR-SACπ planner embodies a safer, more conservative strategy by accounting for environmental uncertainty and optimizing for worst-case scenarios, in contrast to the conventional SAC approach that focuses on maximizing average expected reward, which may not be as robust in uncertain situations.</figcaption>
                            </figure>
                        </div>
                    </div>
                    
                    <!-- Discussion -->
                    <div class="review-card" id="discussion">
                        <h2 class="review-section-title">Discussion</h2>
                        <div class="review-section-content">
                            <p>The discussion provides a critical analysis of the proposed approach:</p>
                            <ul>
                                <li><strong>Trade-Offs in Reward Optimization</strong>:</li>
                                <p>WThe paper highlights that while maximizing the average reward might yield higher overall performance in benign conditions, it can lead to unsafe trajectories when unexpected events occur. By focusing on the worst-case outcome, the conservative RL policies offer a more robust alternative for safety-critical applications.</p>
                                <li><strong>Effectiveness of Quantile Regression</strong>:</li>
                                <p>The integration of quantile regression is shown to be beneficial, although its impact varies between scenarios. In settings with significant uncertainty, the conservative approach clearly outperforms traditional methods, though further tuning might be needed for complex, multi-agent environments.</p>
                                <li><strong>Limitations and Future Directions</strong>:</li>
                                <p>The reliance on simulation (using SUMO) raises questions about the direct transferability of the results to real-world systems. Additionally, while the conservative policies improve safety, they may sometimes lead to overly cautious behavior. The discussion suggests that future research could explore more adaptive methods, including meta-learning and enhanced sensor fusion techniques, to better balance safety and performance.</li></p>
                            </ul>
                        </div>
                    </div>
                    
                    <!-- Conclusion -->
                    <div class="review-card" id="conclusion">
                        <h2 class="review-section-title">Conclusion</h2>
                        <div class="review-section-content">
                            <p>In conclusion, the paper makes a compelling case for rethinking traditional reinforcement learning objectives in the context of autonomous motion planning. By shifting the focus from average reward maximization to worst-case outcome optimization, the authors demonstrate that safer and more robust driving policies can be achieved. The experimental validation using the SUMO simulator in challenging scenarios shows that the proposed modifications—applied to both SAC and DQN frameworks—yield significant improvements in collision avoidance and overall driving behavior. While promising, the study also identifies avenues for further research, particularly in extending the approach to more diverse and complex real-world situations.</p>
                        </div>
                    </div>
                    
                    <!-- Reviewer notes -->
                    <div class="review-card reviewer-notes">
                        <h2 class="review-section-title">Reviewer Notes</h2>
                        <div class="review-section-content">
                            <!-- 요약 섹션 -->
                            <div class="review-summary-section">
                                <h3 class="summary-section-title">Key Points</h3>
                                
                                <div class="review-line">
                                    <div class="line-number">1</div>
                                    <div class="line-content">
                                        <p>The paper introduces a novel RL framework that focuses on optimizing the worst-case outcome, which is crucial for safety in autonomous driving.</p>
                                    </div>
                                </div>
                                
                                <div class="review-line">
                                    <div class="line-number">2</div>
                                    <div class="line-content">
                                        <p>It successfully integrates quantile regression into both value-based and actor-critic RL methods to provide a conservative evaluation of actions.</p>
                                    </div>
                                </div>
                                
                                <div class="review-line">
                                    <div class="line-number">3</div>
                                    <div class="line-content">
                                        <p>Experimental results in simulation demonstrate that the conservative RL policies can significantly reduce collision rates in uncertain scenarios.</p>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Gap 섹션 -->
                            <div class="review-gap-section">
                                <h3 class="gap-section-title">Extended Analysis</h3>
                                
                                <div class="gap-line">
                                    <div class="line-number">1</div>
                                    <div class="line-content">
                                        <p>The heavy reliance on simulated environments (SUMO) raises concerns about the method’s scalability and adaptability to real-world driving conditions.</p>
                                    </div>
                                </div>
                                
                                <div class="gap-line">
                                    <div class="line-number">2</div>
                                    <div class="line-content">
                                        <p>While quantile regression is a creative approach, its performance benefits appear to be scenario-dependent, suggesting that further research is needed to generalize the method.</p>
                                    </div>
                                </div>
                                
                                <div class="gap-line">
                                    <div class="line-number">3</div>
                                    <div class="line-content">
                                        <p>Future work should investigate the integration of additional sensory inputs and multi-agent dynamics to better capture the complexities of real autonomous driving environments.</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <!-- Sidebar -->
                <div class="review-sidebar">
                    <!-- TOC -->
                    <div class="sidebar-card toc-card">
                        <h3 class="sidebar-title">Table of Contents</h3>
                        <ul class="toc-list">
                            <li><a href="#introduction" class="toc-link">Introduction</a></li>
                            <li><a href="#methodology" class="toc-link">Methodology</a></li>
                            <li><a href="#results" class="toc-link">Results</a></li>
                            <li><a href="#discussion" class="toc-link">Discussion</a></li>
                            <li><a href="#conclusion" class="toc-link">Conclusion</a></li>
                            <li><a href="#references" class="toc-link">References</a></li>
                        </ul>
                    </div>
                    
                    <!-- Paper details -->
                    <div class="sidebar-card details-card">
                        <h3 class="sidebar-title">Paper Details</h3>
                        <ul class="details-list">
                            <li class="details-item">
                                <span class="detail-label">Published:</span>
                                <span class="detail-value">October 2021</span>
                            </li>
                            <li class="details-item">
                                <span class="detail-label">Journal:</span>
                                <span class="detail-value">IROS</span>
                            </li>
                            <li class="details-item">
                                <span class="detail-label">Volume:</span>
                                <span class="detail-value">6, Issue 3</span>
                            </li>
                            <li class="details-item">
                                <span class="detail-label">DOI:</span>
                                <span class="detail-value"><a href="
                                    10.1109/IROS51168.2021.9636480" target="_blank">10.1109/IROS51168.2021.9636480</a></span>
                            </li>
                            <li class="details-item">
                                <span class="detail-label">Citations:</span>
                                <span class="detail-value">27+</span>
                            </li>
                        </ul>
                    </div>
                    
                    <!-- Related papers -->
                    <div class="sidebar-card related-card">
                        <h3 class="sidebar-title">Related Papers</h3>
                        <div class="related-papers">
                            <a href="#" class="related-paper">
                                <span class="related-paper-title">A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data</span>
                                <div class="related-paper-meta">
                                    <span class="related-paper-authors">Remonda et al.</span>
                                    <span class="related-paper-year">2024</span>
                                </div>
                            </a>
                            <a href="#" class="related-paper">
                                <span class="related-paper-title">Related paper placeholder 2</span>
                                <div class="related-paper-meta">
                                    <span class="related-paper-authors">Author et al.</span>
                                    <span class="related-paper-year">2023</span>
                                </div>
                            </a>
                            <a href="#" class="related-paper">
                                <span class="related-paper-title">Related paper placeholder 3</span>
                                <div class="related-paper-meta">
                                    <span class="related-paper-authors">Author et al.</span>
                                    <span class="related-paper-year">2022</span>
                                </div>
                            </a>
                        </div>
                    </div>
                    
                    <!-- Tags -->
                    <div class="sidebar-card tags-card">
                        <h3 class="sidebar-title">Tags</h3>
                        <div class="sidebar-tags">
                            <a href="#" class="tag">Motion Planning</a>
                            <a href="#" class="tag">Reinforcement Learning</a>
                            <a href="#" class="tag">Distributional RL</a>
                            <a href="#" class="tag">Soft-Actor-Critic</a>
                            <a href="#" class="tag">Occlusion</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <div class="logo">
                        <svg class="logo-icon" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <rect width="40" height="40" rx="8" fill="#F4F0FF"/>
                            <path d="M20 8L32 14.5V27.5L20 34L8 27.5V14.5L20 8Z" stroke="#6E56CF" stroke-width="2"/>
                            <path d="M20 21V34M20 8V21M8 14.5L20 21L32 14.5" stroke="#6E56CF" stroke-width="2"/>
                            <circle cx="20" cy="21" r="3" fill="#6E56CF"/>
                        </svg>
                        <span class="logo-text">Paper Reviews</span>
                    </div>
                    <p class="footer-tagline">
                        Illuminating research in AI and robotics through comprehensive paper reviews.
                    </p>
                </div>
                
                <div class="footer-nav">
                    <div class="footer-nav-group">
                        <h4 class="footer-heading">Navigation</h4>
                        <ul class="footer-links">
                            <li><a href="https://paper-reviews97.github.io">Home</a></li>
                            <li><a href="https://paper-reviews97.github.io/#categories">Categories</a></li>
                            <li><a href="https://paper-reviews97.github.io#featured">Featured</a></li>
                            <li><a href="https://leejisue.github.io/">About</a></li>
                        </ul>
                    </div>
                    
                    <div class="footer-nav-group">
                        <h4 class="footer-heading">Categories</h4>
                        <ul class="footer-links">
                            <li><a href="https://paper-reviews97.github.io/fundamental-networks/">Fundamental Networks</a></li>
                            <li><a href="../index.html" class="active-footer-link">Racing</a></li>
                            <li><a href="#">Reinforcement Learning</a></li>
                            <li><a href="#">Robotics</a></li>
                        </ul>
                    </div>
                    
                    <div class="footer-nav-group">
                        <h4 class="footer-heading">Connect</h4>
                        <ul class="footer-links">
                            <li><a href="#">Twitter</a></li>
                            <li><a href="https://leejisue.github.io/">GitHub</a></li>
                            <li><a href="#">LinkedIn</a></li>
                            <li><a href="#">Email</a></li>
                        </ul>
                    </div>
                </div>
            </div>
            
            <div class="footer-bottom">
                <div class="copyright">
                    © 2025 Paper Reviews. All rights reserved. Unauthorized reproduction or distribution of any materials on this site is strictly prohibited. <br>
                    All content on this website is the intellectual property of Bruno Lee. For inquiries, please contact: <a href="mailto:brunoleej@gmail.com">brunoleej@gmail.com</a>
                </div>
            </div>
        </div>
    </footer>

    <script src="./js/paper-review.js"></script>
    <script src="./js/paper-review.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>